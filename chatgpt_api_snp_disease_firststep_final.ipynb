{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29290e56-f9fe-4863-a463-6354c38b528a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to your SNP-disease file:  C:\\Users\\Shaoyi Zhang\\Desktop\\Jupyter NoteBook\\snp_disease.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directory: snp_disease_results\n",
      "\n",
      "Processing pair 1: rs429358 - alzheimer\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing: SNP rs429358 association with alzheimer\n",
      "--------------------------------------------------\n",
      "Iteration 1: Model is making tool calls...\n",
      "Searching GWAS Catalog for SNP: rs429358\n",
      "Fetching information for SNP: rs429358\n",
      "\n",
      "Final response received.\n",
      "Completed processing SNP rs429358 association with alzheimer. Results saved to rs429358_alzheimer.txt\n",
      "\n",
      "Processing pair 2: rs2476601 - rheumatoid arthritis\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing: SNP rs2476601 association with rheumatoid arthritis\n",
      "--------------------------------------------------\n",
      "Iteration 1: Model is making tool calls...\n",
      "Searching GWAS Catalog for SNP: rs2476601\n",
      "Fetching information for SNP: rs2476601\n",
      "\n",
      "Final response received.\n",
      "Completed processing SNP rs2476601 association with rheumatoid arthritis. Results saved to rs2476601_rheumatoid_arthritis.txt\n",
      "\n",
      "Processing pair 3: rs12345 - diabetes\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing: SNP rs12345 association with diabetes\n",
      "--------------------------------------------------\n",
      "Iteration 1: Model is making tool calls...\n",
      "Searching GWAS Catalog for SNP: rs12345\n",
      "Fetching information for SNP: rs12345\n",
      "\n",
      "Final response received.\n",
      "Completed processing SNP rs12345 association with diabetes. Results saved to rs12345_diabetes.txt\n",
      "\n",
      "All SNP-disease pairs have been processed.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Filter BeautifulSoup warnings about XML being parsed as HTML\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "\n",
    "# Check and install required dependencies\n",
    "try:\n",
    "    import lxml\n",
    "except ImportError:\n",
    "    print(\"Installing lxml parser for BeautifulSoup...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\"])\n",
    "    print(\"lxml installed successfully\")\n",
    "\n",
    "# Initialize the client with your API key\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "# Function to search PubMed for scientific articles\n",
    "def search_pubmed(query):\n",
    "    try:\n",
    "        # Search PubMed using E-utilities\n",
    "        base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "        \n",
    "        # First get IDs\n",
    "        search_url = f\"{base_url}esearch.fcgi\"\n",
    "        params = {\n",
    "            'db': 'pubmed',\n",
    "            'term': query,\n",
    "            'retmode': 'json',\n",
    "            'retmax': 5\n",
    "        }\n",
    "        \n",
    "        response = requests.get(search_url, params=params)\n",
    "        search_data = response.json()\n",
    "        \n",
    "        if 'esearchresult' in search_data and 'idlist' in search_data['esearchresult']:\n",
    "            id_list = search_data['esearchresult']['idlist']\n",
    "            \n",
    "            if not id_list:\n",
    "                return {\"results\": [], \"message\": \"No PubMed articles found.\"}\n",
    "                \n",
    "            # Then get summaries\n",
    "            summary_url = f\"{base_url}esummary.fcgi\"\n",
    "            params = {\n",
    "                'db': 'pubmed',\n",
    "                'id': ','.join(id_list),\n",
    "                'retmode': 'json'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(summary_url, params=params)\n",
    "            summary_data = response.json()\n",
    "            \n",
    "            results = []\n",
    "            if 'result' in summary_data:\n",
    "                for pmid in id_list:\n",
    "                    if pmid in summary_data['result']:\n",
    "                        article = summary_data['result'][pmid]\n",
    "                        title = article.get('title', 'No title available')\n",
    "                        \n",
    "                        # Create result object\n",
    "                        results.append({\n",
    "                            'title': title,\n",
    "                            'link': f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\",\n",
    "                            'pmid': pmid,\n",
    "                            'authors': ', '.join([author.get('name', '') for author in article.get('authors', []) if 'name' in author]),\n",
    "                            'journal': article.get('fulljournalname', 'Journal not specified'),\n",
    "                            'publication_date': article.get('pubdate', 'Date not specified')\n",
    "                        })\n",
    "            \n",
    "            return {\"results\": results, \"message\": f\"Found {len(results)} articles on PubMed.\"}\n",
    "        else:\n",
    "            return {\"results\": [], \"message\": \"No PubMed articles found or error in search.\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"message\": \"Error searching PubMed.\"}\n",
    "\n",
    "# Function to search for SNP information\n",
    "def search_snp_info(snp_id):\n",
    "    try:\n",
    "        # Check if the SNP ID is in the correct format (rs followed by numbers)\n",
    "        if not re.match(r'^rs\\d+$', snp_id, re.IGNORECASE) and not re.match(r'^\\d+$', snp_id):\n",
    "            snp_id_for_search = snp_id\n",
    "        else:\n",
    "            # If it's just numbers, add rs prefix\n",
    "            if re.match(r'^\\d+$', snp_id):\n",
    "                snp_id_for_search = f\"rs{snp_id}\"\n",
    "            else:\n",
    "                snp_id_for_search = snp_id\n",
    "                \n",
    "        # Normalize to lowercase rs followed by uppercase RS\n",
    "        snp_id_for_search = snp_id_for_search.lower()\n",
    "        \n",
    "        # Search NCBI dbSNP database using E-utilities\n",
    "        base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "        \n",
    "        # First search for the SNP\n",
    "        search_url = f\"{base_url}esearch.fcgi\"\n",
    "        params = {\n",
    "            'db': 'snp',\n",
    "            'term': snp_id_for_search,\n",
    "            'retmode': 'json',\n",
    "            'retmax': 1\n",
    "        }\n",
    "        \n",
    "        response = requests.get(search_url, params=params)\n",
    "        search_data = response.json()\n",
    "        \n",
    "        if 'esearchresult' in search_data and 'idlist' in search_data['esearchresult'] and search_data['esearchresult']['idlist']:\n",
    "            snp_db_id = search_data['esearchresult']['idlist'][0]\n",
    "            \n",
    "            # Then get summary\n",
    "            summary_url = f\"{base_url}esummary.fcgi\"\n",
    "            params = {\n",
    "                'db': 'snp',\n",
    "                'id': snp_db_id,\n",
    "                'retmode': 'json'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(summary_url, params=params)\n",
    "            summary_data = response.json()\n",
    "            \n",
    "            if 'result' in summary_data and snp_db_id in summary_data['result']:\n",
    "                snp_data = summary_data['result'][snp_db_id]\n",
    "                \n",
    "                # Extract relevant information\n",
    "                snp_info = {\n",
    "                    'snp_id': snp_id_for_search,\n",
    "                    'ncbi_id': snp_db_id,\n",
    "                    'chr_loc': snp_data.get('chrpos', 'Unknown location'),\n",
    "                    'gene_loc': snp_data.get('genes', 'Unknown gene'),\n",
    "                    'alleles': snp_data.get('allele', 'Unknown alleles'),\n",
    "                    'assembly': snp_data.get('assembly', 'Unknown assembly'),\n",
    "                    'functional_class': snp_data.get('func', 'Unknown function'),\n",
    "                    'ncbi_link': f\"https://www.ncbi.nlm.nih.gov/snp/{snp_id_for_search}\"\n",
    "                }\n",
    "                \n",
    "                return {\"info\": snp_info, \"message\": f\"Found SNP information for {snp_id_for_search}\"}\n",
    "            else:\n",
    "                return {\"info\": {}, \"message\": f\"No detailed information found for SNP {snp_id_for_search}\"}\n",
    "        else:\n",
    "            # If direct search fails, try to search with some alternative patterns\n",
    "            alternate_search_terms = [\n",
    "                snp_id.upper() if snp_id.islower() else snp_id.lower(),\n",
    "                f\"rs{snp_id}\" if not snp_id.startswith(\"rs\") else snp_id[2:],\n",
    "                f\"RS{snp_id}\" if not snp_id.startswith(\"RS\") else snp_id[2:]\n",
    "            ]\n",
    "            \n",
    "            for alt_term in alternate_search_terms:\n",
    "                search_url = f\"{base_url}esearch.fcgi\"\n",
    "                params = {\n",
    "                    'db': 'snp',\n",
    "                    'term': alt_term,\n",
    "                    'retmode': 'json',\n",
    "                    'retmax': 1\n",
    "                }\n",
    "                \n",
    "                response = requests.get(search_url, params=params)\n",
    "                search_data = response.json()\n",
    "                \n",
    "                if 'esearchresult' in search_data and 'idlist' in search_data['esearchresult'] and search_data['esearchresult']['idlist']:\n",
    "                    return {\"info\": {}, \"message\": f\"SNP {snp_id} might be found as {alt_term}, please try that format\"}\n",
    "            \n",
    "            # If all searches fail, check GWAS Catalog directly\n",
    "            return {\n",
    "                \"info\": {},\n",
    "                \"message\": f\"SNP {snp_id} not found in NCBI dbSNP. You might want to search the GWAS Catalog directly at https://www.ebi.ac.uk/gwas/variants/{snp_id}\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"message\": f\"Error searching SNP information for {snp_id}\"}\n",
    "\n",
    "# Function to search GWAS Catalog for SNP-disease associations\n",
    "def search_gwas_catalog(snp_id):\n",
    "    try:\n",
    "        # Format SNP ID properly\n",
    "        if re.match(r'^\\d+$', snp_id):\n",
    "            snp_id_formatted = f\"rs{snp_id}\"\n",
    "        else:\n",
    "            snp_id_formatted = snp_id\n",
    "            \n",
    "        # Use EBI GWAS Catalog API\n",
    "        gwas_api_url = f\"https://www.ebi.ac.uk/gwas/rest/api/singleNucleotidePolymorphisms/{snp_id_formatted.lower()}/associations\"\n",
    "        \n",
    "        headers = {\n",
    "            'Accept': 'application/json',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(gwas_api_url, headers=headers)\n",
    "        \n",
    "        # If successful API response\n",
    "        if response.status_code == 200:\n",
    "            gwas_data = response.json()\n",
    "            \n",
    "            if '_embedded' in gwas_data and 'associations' in gwas_data['_embedded']:\n",
    "                associations = gwas_data['_embedded']['associations']\n",
    "                \n",
    "                results = []\n",
    "                for assoc in associations:\n",
    "                    # Extract relevant information\n",
    "                    study = assoc.get('study', {})\n",
    "                    strongest_risk_alleles = assoc.get('strongestRiskAlleles', [{}])\n",
    "                    loci = assoc.get('loci', [{}])\n",
    "                    \n",
    "                    result = {\n",
    "                        'disease_trait': study.get('diseaseTrait', {}).get('trait', 'Unknown trait'),\n",
    "                        'p_value': assoc.get('pvalue', 'Unknown p-value'),\n",
    "                        'risk_allele': strongest_risk_alleles[0].get('riskAlleleName', 'Unknown risk allele') if strongest_risk_alleles else 'Unknown',\n",
    "                        'odds_ratio': assoc.get('orPerCopyNum', 'Unknown odds ratio'),\n",
    "                        'beta_coefficient': assoc.get('betaNum', 'Unknown beta coefficient'),\n",
    "                        'study_id': study.get('accessionId', 'Unknown study ID'),\n",
    "                        'pubmed_id': study.get('publicationInfo', {}).get('pubmedId', 'Unknown PMID'),\n",
    "                        'author': study.get('publicationInfo', {}).get('author', {}).get('fullname', 'Unknown author'),\n",
    "                        'publication_date': study.get('publicationInfo', {}).get('publicationDate', 'Unknown date'),\n",
    "                        'mapped_gene': \", \".join([l.get('gene', {}).get('geneName', 'Unknown gene') for l in loci if 'gene' in l and l['gene']]),\n",
    "                        'study_link': f\"https://www.ebi.ac.uk/gwas/studies/{study.get('accessionId')}\" if 'accessionId' in study else None\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "                \n",
    "                return {\"results\": results, \"message\": f\"Found {len(results)} GWAS associations for {snp_id_formatted}\"}\n",
    "            else:\n",
    "                return {\"results\": [], \"message\": f\"No GWAS associations found for {snp_id_formatted}\"}\n",
    "        else:\n",
    "            # Try alternate format with uppercase RS\n",
    "            if snp_id_formatted.startswith(\"rs\"):\n",
    "                alternate_id = f\"RS{snp_id_formatted[2:]}\"\n",
    "                gwas_api_url = f\"https://www.ebi.ac.uk/gwas/rest/api/singleNucleotidePolymorphisms/{alternate_id}/associations\"\n",
    "                response = requests.get(gwas_api_url, headers=headers)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    return {\"results\": [], \"message\": f\"SNP may be found as {alternate_id} in GWAS Catalog. Please try that format.\"}\n",
    "            \n",
    "            return {\"results\": [], \"message\": f\"No information found for {snp_id_formatted} in GWAS Catalog (Status code: {response.status_code})\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"message\": f\"Error searching GWAS Catalog for {snp_id}\"}\n",
    "\n",
    "# Function to search the web (improved version)\n",
    "def search_web(query):\n",
    "    try:\n",
    "        # Prepare search URL with the query\n",
    "        search_url = f\"https://www.google.com/search?q={query.replace(' ', '+')}\"\n",
    "        \n",
    "        # Request headers to mimic a browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Make the request\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract search results\n",
    "        results = []\n",
    "        \n",
    "        # Process Google search results\n",
    "        for g in soup.select('div.g'):\n",
    "            # Extract title\n",
    "            title_elem = g.select_one('h3')\n",
    "            if not title_elem:\n",
    "                continue\n",
    "                \n",
    "            title = title_elem.get_text()\n",
    "            \n",
    "            # Extract URL\n",
    "            link_elem = g.select_one('a')\n",
    "            if not link_elem or 'href' not in link_elem.attrs:\n",
    "                continue\n",
    "                \n",
    "            link = link_elem['href']\n",
    "            if link.startswith('/url?'):\n",
    "                link = re.search(r'/url\\?q=([^&]+)', link).group(1)\n",
    "            elif not link.startswith('http'):\n",
    "                continue\n",
    "                \n",
    "            # Extract snippet\n",
    "            snippet_elem = g.select_one('.VwiC3b, .st')\n",
    "            snippet = snippet_elem.get_text() if snippet_elem else \"No description available\"\n",
    "            \n",
    "            # Check if this is a scholarly/SNP/GWAS source\n",
    "            is_scholarly = any(domain in link.lower() for domain in [\n",
    "                'nih.gov', 'ncbi.nlm', 'pubmed', 'nature.com', 'sciencedirect',\n",
    "                'scholar.google', 'researchgate', 'academic', 'science', 'journal',\n",
    "                'snp', 'gwas', 'genome', 'variant', 'polymorphism', 'ebi.ac.uk',\n",
    "                'genetics', 'genomic', 'allele'\n",
    "            ])\n",
    "            \n",
    "            # Add to results\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'snippet': snippet,\n",
    "                'is_scholarly': is_scholarly\n",
    "            })\n",
    "            \n",
    "            # Limit results\n",
    "            if len(results) >= 5:\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            \"results\": results,\n",
    "            \"message\": f\"Found {len(results)} web results for '{query}'.\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"message\": \"Error during web search.\"}\n",
    "\n",
    "# Function to fetch abstracts for a specific PubMed article\n",
    "def fetch_pubmed_abstract(pmid):\n",
    "    try:\n",
    "        # Use E-utilities to fetch the abstract\n",
    "        base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "        fetch_url = f\"{base_url}efetch.fcgi\"\n",
    "        params = {\n",
    "            'db': 'pubmed',\n",
    "            'id': pmid,\n",
    "            'retmode': 'xml'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(fetch_url, params=params)\n",
    "        \n",
    "        # Try multiple parser approaches to handle different XML formats\n",
    "        try:\n",
    "            # First attempt with lxml-xml\n",
    "            soup = BeautifulSoup(response.text, features=\"lxml-xml\")\n",
    "        except Exception as e:\n",
    "            print(f\"XML parsing with lxml-xml failed: {e}\")\n",
    "            try:\n",
    "                # Second attempt with xml parser\n",
    "                soup = BeautifulSoup(response.text, features=\"xml\")\n",
    "            except Exception as e:\n",
    "                print(f\"XML parsing with xml failed: {e}\")\n",
    "                # Fall back to html parser as last resort\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the abstract text\n",
    "        abstract_element = soup.find('AbstractText')\n",
    "        if abstract_element:\n",
    "            abstract = abstract_element.get_text()\n",
    "        else:\n",
    "            # Try alternative approach if the first method fails\n",
    "            abstract_sections = soup.find_all('AbstractText')\n",
    "            if abstract_sections:\n",
    "                abstract = \" \".join([section.get_text() for section in abstract_sections])\n",
    "            else:\n",
    "                abstract = \"Abstract not available for this article.\"\n",
    "            \n",
    "        title_element = soup.find('ArticleTitle')\n",
    "        title = title_element.get_text() if title_element else \"Title not available\"\n",
    "        \n",
    "        return {\n",
    "            \"pmid\": pmid,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"url\": f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"pmid\": pmid, \"message\": \"Error fetching abstract.\"}\n",
    "\n",
    "# Function to fetch content from a webpage\n",
    "def fetch_webpage(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        # Get text\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Truncate if too long (OpenAI has token limits)\n",
    "        max_length = 8000\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length] + \"... [Content truncated due to length]\"\n",
    "            \n",
    "        return {\"content\": text, \"url\": url}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"url\": url}\n",
    "\n",
    "# Function to process a SNP-disease pair and store results\n",
    "def process_snp_disease_pair(snp, disease):\n",
    "    print(f\"\\n{'-' * 50}\")\n",
    "    print(f\"Processing: SNP {snp} association with {disease}\")\n",
    "    print(f\"{'-' * 50}\")\n",
    "    \n",
    "    # Create a safe filename\n",
    "    filename = f\"{snp.replace(' ', '_')}_{disease.replace(' ', '_')}.txt\"\n",
    "    \n",
    "    # Define available functions\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"search_pubmed\",\n",
    "                \"description\": \"Search PubMed for scientific articles about SNPs and diseases\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query for PubMed\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"search_snp_info\",\n",
    "                \"description\": \"Search for information about a specific SNP in NCBI dbSNP database\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"snp_id\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The SNP ID to search for (e.g., rs12345)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"snp_id\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"search_gwas_catalog\",\n",
    "                \"description\": \"Search the GWAS Catalog for associations related to a specific SNP\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"snp_id\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The SNP ID to search for (e.g., rs12345)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"snp_id\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"search_web\",\n",
    "                \"description\": \"Search the web for general information\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"fetch_pubmed_abstract\",\n",
    "                \"description\": \"Fetch the abstract for a specific PubMed article by its ID\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"pmid\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The PubMed ID (PMID) of the article\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"pmid\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"fetch_webpage\",\n",
    "                \"description\": \"Fetch the content of a webpage\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"url\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The URL to fetch\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"url\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Initialize conversation with a specific query about the SNP and disease\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in genetic epidemiology, GWAS, and the analysis of genetic variants in disease risk. You can search for SNP information in databases, analyze GWAS data, and interpret the significance of genetic variants in disease contexts.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze the association between SNP {snp} and {disease} based on GWAS results. Research this variant's location, nearby genes, allele frequencies, functional consequences, existing evidence for its involvement in {disease}, and biological mechanisms that might explain this association. Provide a comprehensive analysis including statistical significance, odds ratios, potential causal pathways, and implications for disease risk prediction or treatment.\"}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Create output file\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(f\"Analysis of SNP {snp} association with {disease}\\n\")\n",
    "            output_file.write(f\"{'-' * 50}\\n\\n\")\n",
    "            \n",
    "            # Get initial response from the model\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                tools=tools,\n",
    "                tool_choice=\"auto\"\n",
    "            )\n",
    "            \n",
    "            response_message = response.choices[0].message\n",
    "            \n",
    "            # Log all messages to the file for complete conversation tracking\n",
    "            def log_message(role, content):\n",
    "                output_file.write(f\"[{role.upper()}]: {content}\\n\\n\")\n",
    "                output_file.write(f\"{'-' * 30}\\n\\n\")\n",
    "            \n",
    "            # Check if the model wants to call a function\n",
    "            iteration = 0\n",
    "            max_iterations = 10  # Limit to prevent infinite loops\n",
    "            \n",
    "            while hasattr(response_message, 'tool_calls') and response_message.tool_calls and iteration < max_iterations:\n",
    "                iteration += 1\n",
    "                print(f\"Iteration {iteration}: Model is making tool calls...\")\n",
    "                \n",
    "                # Add the assistant's message to the history\n",
    "                messages.append(response_message)\n",
    "                log_message(\"assistant\", f\"Making the following tool calls: {[tc.function.name for tc in response_message.tool_calls]}\")\n",
    "                \n",
    "                # Process each tool call\n",
    "                for tool_call in response_message.tool_calls:\n",
    "                    function_name = tool_call.function.name\n",
    "                    function_args = json.loads(tool_call.function.arguments)\n",
    "                    \n",
    "                    # Call the appropriate function\n",
    "                    function_response = None\n",
    "                    if function_name == \"search_pubmed\":\n",
    "                        query = function_args.get(\"query\")\n",
    "                        print(f\"Searching PubMed for: {query}\")\n",
    "                        function_response = search_pubmed(query)\n",
    "                        log_message(\"tool\", f\"search_pubmed query: {query}\\nResults: {json.dumps(function_response, indent=2)}\")\n",
    "                    elif function_name == \"search_snp_info\":\n",
    "                        snp_id = function_args.get(\"snp_id\")\n",
    "                        print(f\"Fetching information for SNP: {snp_id}\")\n",
    "                        function_response = search_snp_info(snp_id)\n",
    "                        log_message(\"tool\", f\"search_snp_info for: {snp_id}\\nResults: {json.dumps(function_response, indent=2)}\")\n",
    "                    elif function_name == \"search_gwas_catalog\":\n",
    "                        snp_id = function_args.get(\"snp_id\")\n",
    "                        print(f\"Searching GWAS Catalog for SNP: {snp_id}\")\n",
    "                        function_response = search_gwas_catalog(snp_id)\n",
    "                        log_message(\"tool\", f\"search_gwas_catalog for: {snp_id}\\nResults: {json.dumps(function_response, indent=2)}\")\n",
    "                    elif function_name == \"search_web\":\n",
    "                        query = function_args.get(\"query\")\n",
    "                        print(f\"Searching the web for: {query}\")\n",
    "                        function_response = search_web(query)\n",
    "                        log_message(\"tool\", f\"search_web query: {query}\\nResults: {json.dumps(function_response, indent=2)}\")\n",
    "                    elif function_name == \"fetch_pubmed_abstract\":\n",
    "                        pmid = function_args.get(\"pmid\")\n",
    "                        print(f\"Fetching abstract for PubMed ID: {pmid}\")\n",
    "                        function_response = fetch_pubmed_abstract(pmid)\n",
    "                        log_message(\"tool\", f\"fetch_pubmed_abstract pmid: {pmid}\\nResults: {json.dumps(function_response, indent=2)}\")\n",
    "                    elif function_name == \"fetch_webpage\":\n",
    "                        url = function_args.get(\"url\")\n",
    "                        print(f\"Fetching webpage: {url}\")\n",
    "                        function_response = fetch_webpage(url)\n",
    "                        # Log URL and truncated content to avoid huge files\n",
    "                        content_preview = function_response.get(\"content\", \"\")[:1000] + \"...\" if \"content\" in function_response else \"\"\n",
    "                        log_message(\"tool\", f\"fetch_webpage url: {url}\\nContent preview: {content_preview}\")\n",
    "                    \n",
    "                    # Add the function result to the messages\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": json.dumps(function_response)\n",
    "                    })\n",
    "                \n",
    "                # Get the next response after tool use\n",
    "                next_response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=messages,\n",
    "                    tools=tools,\n",
    "                    tool_choice=\"auto\"\n",
    "                )\n",
    "                \n",
    "                response_message = next_response.choices[0].message\n",
    "                \n",
    "                # If no more tool calls, break the loop\n",
    "                if not hasattr(response_message, 'tool_calls') or not response_message.tool_calls:\n",
    "                    ai_response = response_message.content\n",
    "                    print(\"\\nFinal response received.\")\n",
    "                    log_message(\"final_response\", ai_response)\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "                    break\n",
    "            \n",
    "            # Add final summary if we reached max iterations\n",
    "            if iteration >= max_iterations:\n",
    "                print(\"Reached maximum number of iterations. Requesting final summary...\")\n",
    "                final_prompt = {\"role\": \"user\", \"content\": \"Please provide a final comprehensive summary of all the information you've gathered about this SNP-disease relationship, including statistical evidence, functional impact, and potential mechanisms of action.\"}\n",
    "                messages.append(final_prompt)\n",
    "                log_message(\"user\", final_prompt[\"content\"])\n",
    "                \n",
    "                final_response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=messages\n",
    "                )\n",
    "                \n",
    "                final_answer = final_response.choices[0].message.content\n",
    "                log_message(\"final_summary\", final_answer)\n",
    "            \n",
    "            print(f\"Completed processing SNP {snp} association with {disease}. Results saved to {filename}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing SNP {snp} for {disease}: {e}\")\n",
    "        # Write error to file\n",
    "        with open(filename, \"a\", encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(f\"ERROR: {str(e)}\\n\")\n",
    "\n",
    "# Main function to read input file and process each pair\n",
    "def process_input_file(input_file_path):\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = \"snp_disease_results\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created output directory: {output_dir}\")\n",
    "        \n",
    "        # Change to output directory\n",
    "        os.chdir(output_dir)\n",
    "        \n",
    "        # Read the input file\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            # Determine if the file is tab-separated or comma-separated\n",
    "            first_line = file.readline().strip()\n",
    "            if '\\t' in first_line:\n",
    "                delimiter = '\\t'\n",
    "            else:\n",
    "                delimiter = ','\n",
    "            \n",
    "            # Return to the beginning of the file\n",
    "            file.seek(0)\n",
    "            \n",
    "            # Create a CSV reader\n",
    "            reader = csv.reader(file, delimiter=delimiter)\n",
    "            \n",
    "            # Process each row\n",
    "            for i, row in enumerate(reader):\n",
    "                if len(row) >= 2:\n",
    "                    snp = row[0].strip()\n",
    "                    disease = row[1].strip()\n",
    "                    \n",
    "                    if snp and disease:  # Ensure neither is empty\n",
    "                        print(f\"\\nProcessing pair {i+1}: {snp} - {disease}\")\n",
    "                        process_snp_disease_pair(snp, disease)\n",
    "                    else:\n",
    "                        print(f\"Skipping row {i+1}: Missing SNP or disease\")\n",
    "                else:\n",
    "                    print(f\"Skipping row {i+1}: Insufficient columns\")\n",
    "                \n",
    "        print(\"\\nAll SNP-disease pairs have been processed.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing input file: {e}\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = input(\"Enter the path to your SNP-disease file: \")\n",
    "    process_input_file(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32455429-e81c-4e81-8288-07b5f75a4b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
